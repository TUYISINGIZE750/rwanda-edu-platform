"""
Rwanda Education Platform - Server Monitor & Keep-Alive
Comprehensive monitoring with alerts and analytics
"""
import requests
import time
import json
import os
from datetime import datetime, timedelta
from typing import Dict, List
import smtplib
from email.mime.text import MimeText
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('server_monitor.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ServerMonitor:
    def __init__(self):
        self.base_url = "https://rwanda-edu-platform.onrender.com"
        self.frontend_url = "https://tssanywhere.pages.dev"
        self.check_interval = 300  # 5 minutes
        self.keep_alive_interval = 480  # 8 minutes
        self.last_keep_alive = datetime.now() - timedelta(minutes=10)
        
        # Monitoring data
        self.status_history = []
        self.downtime_alerts = []
        self.performance_data = []
        
        # Load previous data if exists
        self.load_monitoring_data()
    
    def save_monitoring_data(self):
        """Save monitoring data to file"""
        data = {\n            \"status_history\": self.status_history[-1000:],  # Keep last 1000 entries\n            \"downtime_alerts\": self.downtime_alerts[-100:],  # Keep last 100 alerts\n            \"performance_data\": self.performance_data[-1000:]  # Keep last 1000 entries\n        }\n        \n        try:\n            with open('monitoring_data.json', 'w') as f:\n                json.dump(data, f, default=str, indent=2)\n        except Exception as e:\n            logger.error(f\"Failed to save monitoring data: {e}\")\n    \n    def load_monitoring_data(self):\n        \"\"\"Load previous monitoring data\"\"\"\n        try:\n            if os.path.exists('monitoring_data.json'):\n                with open('monitoring_data.json', 'r') as f:\n                    data = json.load(f)\n                    self.status_history = data.get('status_history', [])\n                    self.downtime_alerts = data.get('downtime_alerts', [])\n                    self.performance_data = data.get('performance_data', [])\n                logger.info(f\"Loaded {len(self.status_history)} previous status records\")\n        except Exception as e:\n            logger.error(f\"Failed to load monitoring data: {e}\")\n    \n    def check_server_status(self) -> Dict:\n        \"\"\"Check server status with detailed metrics\"\"\"\n        start_time = time.time()\n        \n        try:\n            # Check main health endpoint\n            response = requests.get(f\"{self.base_url}/health\", timeout=30)\n            response_time = (time.time() - start_time) * 1000  # ms\n            \n            status = {\n                \"timestamp\": datetime.now().isoformat(),\n                \"status\": \"up\" if response.status_code == 200 else \"down\",\n                \"status_code\": response.status_code,\n                \"response_time_ms\": round(response_time, 2),\n                \"server_data\": response.json() if response.status_code == 200 else None\n            }\n            \n            # Additional endpoint checks\n            endpoints_to_check = [\n                \"/api/v1/locations/provinces\",\n                \"/api/v1/auth/test\"\n            ]\n            \n            endpoint_results = []\n            for endpoint in endpoints_to_check:\n                try:\n                    ep_start = time.time()\n                    ep_response = requests.get(f\"{self.base_url}{endpoint}\", timeout=15)\n                    ep_time = (time.time() - ep_start) * 1000\n                    \n                    endpoint_results.append({\n                        \"endpoint\": endpoint,\n                        \"status_code\": ep_response.status_code,\n                        \"response_time_ms\": round(ep_time, 2),\n                        \"success\": ep_response.status_code == 200\n                    })\n                except Exception as e:\n                    endpoint_results.append({\n                        \"endpoint\": endpoint,\n                        \"error\": str(e),\n                        \"success\": False\n                    })\n            \n            status[\"endpoint_checks\"] = endpoint_results\n            \n        except Exception as e:\n            status = {\n                \"timestamp\": datetime.now().isoformat(),\n                \"status\": \"down\",\n                \"error\": str(e),\n                \"response_time_ms\": (time.time() - start_time) * 1000\n            }\n        \n        return status\n    \n    def wake_up_server(self) -> bool:\n        \"\"\"Actively wake up the server\"\"\"\n        logger.info(\"ğŸ”„ Waking up server...\")\n        \n        wake_endpoints = [\n            \"/health\",\n            \"/wake-up\",\n            \"/api/v1/locations/provinces\"\n        ]\n        \n        success_count = 0\n        \n        for endpoint in wake_endpoints:\n            try:\n                response = requests.get(f\"{self.base_url}{endpoint}\", timeout=30)\n                if response.status_code == 200:\n                    success_count += 1\n                    logger.info(f\"âœ… {endpoint} - HTTP {response.status_code}\")\n                else:\n                    logger.warning(f\"âš ï¸ {endpoint} - HTTP {response.status_code}\")\n            except Exception as e:\n                logger.error(f\"âŒ {endpoint} - {str(e)}\")\n        \n        success = success_count > 0\n        if success:\n            logger.info(f\"âœ… Server wake-up successful ({success_count}/{len(wake_endpoints)} endpoints)\")\n            self.last_keep_alive = datetime.now()\n        else:\n            logger.error(\"âŒ Server wake-up failed on all endpoints\")\n        \n        return success\n    \n    def send_alert(self, message: str, alert_type: str = \"warning\"):\n        \"\"\"Send alert (can be extended to email/SMS/Slack)\"\"\"\n        alert = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"type\": alert_type,\n            \"message\": message\n        }\n        \n        self.downtime_alerts.append(alert)\n        \n        # Log alert\n        if alert_type == \"critical\":\n            logger.critical(f\"ğŸš¨ CRITICAL: {message}\")\n        elif alert_type == \"warning\":\n            logger.warning(f\"âš ï¸ WARNING: {message}\")\n        else:\n            logger.info(f\"â„¹ï¸ INFO: {message}\")\n        \n        # Here you can add email/SMS/Slack notifications\n        # self.send_email_alert(message)\n        # self.send_slack_alert(message)\n    \n    def analyze_performance(self):\n        \"\"\"Analyze server performance trends\"\"\"\n        if len(self.status_history) < 10:\n            return\n        \n        recent_statuses = self.status_history[-50:]  # Last 50 checks\n        \n        # Calculate uptime percentage\n        up_count = sum(1 for s in recent_statuses if s.get(\"status\") == \"up\")\n        uptime_percentage = (up_count / len(recent_statuses)) * 100\n        \n        # Calculate average response time\n        response_times = [s.get(\"response_time_ms\", 0) for s in recent_statuses if s.get(\"status\") == \"up\"]\n        avg_response_time = sum(response_times) / len(response_times) if response_times else 0\n        \n        # Performance summary\n        performance = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"uptime_percentage\": round(uptime_percentage, 2),\n            \"avg_response_time_ms\": round(avg_response_time, 2),\n            \"total_checks\": len(recent_statuses),\n            \"successful_checks\": up_count\n        }\n        \n        self.performance_data.append(performance)\n        \n        # Alert on poor performance\n        if uptime_percentage < 90:\n            self.send_alert(f\"Low uptime detected: {uptime_percentage:.1f}% in last {len(recent_statuses)} checks\", \"critical\")\n        elif avg_response_time > 5000:  # 5 seconds\n            self.send_alert(f\"Slow response time: {avg_response_time:.0f}ms average\", \"warning\")\n        \n        logger.info(f\"ğŸ“Š Performance - Uptime: {uptime_percentage:.1f}%, Avg Response: {avg_response_time:.0f}ms\")\n    \n    def run_monitoring_cycle(self):\n        \"\"\"Run a single monitoring cycle\"\"\"\n        # Check server status\n        status = self.check_server_status()\n        self.status_history.append(status)\n        \n        # Log status\n        if status[\"status\"] == \"up\":\n            response_time = status.get(\"response_time_ms\", 0)\n            logger.info(f\"âœ… Server UP - Response: {response_time:.0f}ms\")\n        else:\n            error = status.get(\"error\", \"Unknown error\")\n            logger.error(f\"âŒ Server DOWN - {error}\")\n            self.send_alert(f\"Server is down: {error}\", \"critical\")\n        \n        # Keep-alive if needed\n        time_since_keep_alive = datetime.now() - self.last_keep_alive\n        if time_since_keep_alive.total_seconds() > self.keep_alive_interval:\n            self.wake_up_server()\n        \n        # Analyze performance every 10 checks\n        if len(self.status_history) % 10 == 0:\n            self.analyze_performance()\n        \n        # Save data every 20 checks\n        if len(self.status_history) % 20 == 0:\n            self.save_monitoring_data()\n    \n    def run(self):\n        \"\"\"Main monitoring loop\"\"\"\n        logger.info(\"ğŸš€ Rwanda Education Platform Monitor Started\")\n        logger.info(f\"ğŸ¯ Backend: {self.base_url}\")\n        logger.info(f\"ğŸŒ Frontend: {self.frontend_url}\")\n        logger.info(f\"â° Check Interval: {self.check_interval} seconds\")\n        logger.info(f\"ğŸ”„ Keep-Alive Interval: {self.keep_alive_interval} seconds\")\n        logger.info(\"=\" * 60)\n        \n        # Initial wake-up\n        self.wake_up_server()\n        \n        try:\n            while True:\n                self.run_monitoring_cycle()\n                time.sleep(self.check_interval)\n                \n        except KeyboardInterrupt:\n            logger.info(\"ğŸ›‘ Monitoring stopped by user\")\n            self.save_monitoring_data()\n        except Exception as e:\n            logger.error(f\"ğŸš¨ Monitoring error: {e}\")\n            self.save_monitoring_data()\n            raise\n\ndef main():\n    monitor = ServerMonitor()\n    monitor.run()\n\nif __name__ == \"__main__\":\n    main()